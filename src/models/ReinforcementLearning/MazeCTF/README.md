# Maze CTF
This repo is a demonstration of reinforcement learning in which an agent must find a single prize hidden in a (2D and 3D) maze. The steps are as follows: 

1. Define the maze structure: You'll need to define the maze structure, including the size of the maze, the number of walls, and the position of the prize. You could represent the maze using a 3D array or a graph structure.

2. Define the agent: You'll need to define the agent's actions and state representation. The agent can move in four directions (up, down, left, right) in each of the three dimensions, so there are a total of 12 possible actions. The agent's state representation could include its current position in the maze, as well as any other relevant information, such as whether it has already found the prize.

3. Define the reward function: You'll need to define a reward function that rewards the agent for finding the prize and penalizes it for hitting walls or taking too long to find the prize. For example, you could give the agent a reward of +10 for finding the prize and a penalty of -1 for hitting a wall or taking too many steps.

4. Define the episode termination conditions: You'll need to define the conditions for terminating an episode, such as the agent finding the prize, hitting a wall too many times, or taking too many steps without finding the prize.

5. Implement the environment: Finally, you'll need to implement the environment, which should allow the agent to take actions and receive observations and rewards. The environment should keep track of the agent's current position and update it based on its actions, and it should also calculate the reward the agent receives for each action.

## GAN IMPLEMENTATION
I wanted to incorporate a generative adversarial network into this for this particular demonstration of reinforcement learning; partially out of intellectual curiosity and part out of practical purposes. 

### Methodology
I used a generative adversarial network to generate artificial experiences for the RL agent to learn from, as this way I could augment real-world experiences in the RL agent's training dataset. I accomplished this by training a GAN on the RL agent's current policy, so that it can generate fake experiences that are similar to those that the agent might encounter in the real world. These experiences can then be combined with the real-world experiences to create a larger and more diverse training dataset for the agent to learn from. The agent can then be trained using a combination of the real-world and genearted experiences, using a variety of RL algorithms such as Q-learning or policy gradient methods. The goal here is to optimize the agent's policy based on both the real and generated experiences, so that it can learn to perform better in the real world. The main drawback of this methodology is relying on the quality of the generated data, which can be difficult to control and optimize. I implemented this project as follows: 

1. Generate fake experiences using a GAN: We can train a GAN to generate fake maze environments and corresponding actions taken by a random policy. The GAN can be trained to generate fake experiences that are similar to the real maze environments that the agent will encounter during training.

2. Combine real and fake experiences: We can combine the real maze environments and actions taken by the agent with the fake environments and actions generated by the GAN. This creates a larger and more diverse training dataset for the agent to learn from.

3. Train the RL agent: We can use the combined dataset to train an RL agent using a standard RL algorithm such as Q-learning or policy gradient methods. The goal is to optimize the agent's policy based on both the real and generated experiences, so that it can learn to navigate the maze environment to reach the goal location.

4. Repeat: We can repeat steps 1-3 for multiple epochs, continually improving the GAN's ability to generate more realistic and varied fake experiences, and the RL agent's ability to learn from them.

This approach can help to improve the performance of the RL agent by providing it with a larger and more diverse training dataset. However, it's important to note that the quality of the fake experiences generated by the GAN can greatly impact the effectiveness of this approach. Additionally, training a GAN can be quite computationally expensive, so this approach may not be feasible for all RL problems.